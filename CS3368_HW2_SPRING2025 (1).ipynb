{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NabZ2lZGEUaS"
   },
   "source": [
    "# CS3368, HW2 - SPRING 2024\n",
    "\n",
    "\n",
    "## Topic: Sequential decision making with discrete state and action spaces.\n",
    "\n",
    "In this assignment, you will solve MDP by **Value Iteration** and **Policy Iteration**\n",
    "algorithms with a known dynamics and reward, $T$ and $R$, respectively. \n",
    "\n",
    "The assignment has 2 parts. In Part-1, you are asked to fill in the blanks in the given code, and to complete the given tasks. In Part-2, you are asked to implement the Policy Iteration algorithm using tensor broadcasting (no for loops).\n",
    "\n",
    "You can work in teams of 2 students. Please assign yourselves to the teams on BlackBoard.\n",
    "\n",
    "You can discuss your solutions with other teams, but sharing your code or parts of it with other teams is plagiarism.\n",
    "\n",
    "What to submit on Canvas: 1) a working notebook with the full solution, and 2) its corresponding PDF. \n",
    "\n",
    "\n",
    "Due date : February 14, 2025, 11:59PM \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center> Part-1 </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "lm_n1eT3EUaU"
   },
   "source": [
    "#### Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeIvwWOcEUaV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "You can use your maze from HW1. Make sure that there are multiple paths (at least 3) from the START = 'top left corner' to GOAL='bottom right corner'. If you do not have at least 3 paths, update your maze accordingly\n",
    "</font>\n",
    "\n",
    "#### Design a maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzMMg9NpQ5dD"
   },
   "outputs": [],
   "source": [
    "def get_maze(maze_file):\n",
    "    '''\n",
    "    para1: filename of the maze txt file\n",
    "    return mazes as a numpy array walls: 0 - no wall, 1 - wall\n",
    "    '''\n",
    "    a = open(maze_file, 'r')  \n",
    "    m=[]\n",
    "    for i in a.readlines():\n",
    "        m.append(np.array(i.split(\" \"), dtype=\"int32\"))\n",
    "    return np.array(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "jLFgpnM_EUaW",
    "outputId": "60161d66-2730-4b9b-a534-142462092174"
   },
   "outputs": [],
   "source": [
    "# State Space\n",
    "S=get_maze(\"your_maze_file.txt\")  \n",
    "START = (1,1)\n",
    "GOAL = (25,25)\n",
    "# Action Space\n",
    "A = [\n",
    "    (-1, 0),    # 'up'\n",
    "    (1, 0),     # 'down'\n",
    "    (0, -1),    # 'left'\n",
    "    (0, 1),     # 'right'\n",
    "    (0, 0)      # 'stay'\n",
    "]\n",
    "\n",
    "\n",
    "# Noise\n",
    "ALPHA = [0.2, 0.8]\n",
    "\n",
    "max_it = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoVFsScnEUaW"
   },
   "outputs": [],
   "source": [
    "GRID_SIZE = len(S)\n",
    "# goal state\n",
    "S[GOAL] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8wrecEQEUaX"
   },
   "source": [
    "#### Visualize the maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDXiKR5eEUaX"
   },
   "outputs": [],
   "source": [
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R_xMFadEUaY"
   },
   "outputs": [],
   "source": [
    "plt.imshow(S, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD7ZHnSZEUaY"
   },
   "source": [
    "#### Define a utility function, s_next_calc, which computes the index of the next state given current state and action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFRfCFu1EUaZ"
   },
   "outputs": [],
   "source": [
    "def s_next_calc(s, a):\n",
    "    '''This function returns the agent's next state given action\n",
    "    and current state (assuming the action succeeds).\n",
    "    : param s: Current position of the agent\n",
    "    : param a: action taken by the agent\n",
    "    : returns: New state coordinates in the grid\n",
    "    '''\n",
    "    \n",
    "    return (s[0] + A[a][0], s[1] + A[a][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um83WW0EEUaZ"
   },
   "source": [
    "#### Define a utility function to check if the action at current state leads to a collision with a wall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGOTonn_EUaZ"
   },
   "outputs": [],
   "source": [
    "def hit_wall(curr, action):\n",
    "    '''This function checks if the agent hits any walls. Agent calculates \"hitting the wall\"\n",
    "    with deterministic dynamics\n",
    "    : param curr: Current position of the agent\n",
    "    : param action: Chosen action by the agent\n",
    "    : returns: True/False Binary value to indicate if agent hits a wall\n",
    "    '''\n",
    "    s_new = (\n",
    "        curr[0] + A[action][0],\n",
    "        curr[1] + A[action][1]\n",
    "    )\n",
    "\n",
    "    # Check for grid boundaries\n",
    "    if min(s_new) < 0 or max(s_new) > GRID_SIZE-1:\n",
    "        return True\n",
    "\n",
    "    # Check walls\n",
    "    # 0: 'up':   (-1,  0),\n",
    "    # 1: 'down': ( 1,  0),\n",
    "    # 2: 'left': ( 0, -1),\n",
    "    # 3: 'right':( 0,  1),\n",
    "    # 4: 'stay': ( 0,  0)\n",
    "    if (S[curr]==0 and S[s_new]==1):\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieW0NgQmEUaa"
   },
   "source": [
    "#### Define the reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dZ8GZkiEUaa"
   },
   "outputs": [],
   "source": [
    "def R(s, a):\n",
    "    '''Reward function\n",
    "    : param s: Current state of the agent\n",
    "    : param a: Action the agent takes at the current state\n",
    "    : returns: Reward for the action at current state\n",
    "    '''\n",
    "    if s == GOAL:\n",
    "        return 0\n",
    "    elif hit_wall(s, a):\n",
    "        return -10000\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knmSD8o_EUaa"
   },
   "source": [
    "#### Calculate the transition probabilities to state s_next from current state s upon action a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3wdRpJWEUaa"
   },
   "outputs": [],
   "source": [
    "def Pr(s_next, s, a, alpha):\n",
    "    '''This function returns probabilities for each action at agent's\n",
    "    current state.\n",
    "    :param s: Current state of the agent\n",
    "    :param a: Action the agent takes at the current state\n",
    "    :param alpha: Probability of the agent to take a random action instead of the action a\n",
    "    :returns : Transition probability for the action at current state\n",
    "    '''\n",
    "    # can't move once reached the GOAL\n",
    "    if s == GOAL:\n",
    "        if s_next == s:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # If wall hit, next state is current state \n",
    "    if hit_wall(s, a):\n",
    "        # Legal action with s = s'\n",
    "        if s_next == s:\n",
    "            return 1\n",
    "        # Illegal action with s != s'\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Legal action with adjacent s and s'\n",
    "    if s_next == s_next_calc(s, a):\n",
    "        return 1 - alpha\n",
    "    else:\n",
    "        # Agent moves to another adjacent state instead of s' due to noise in a.\n",
    "        # Generate all other neighbors of s by applying actions other than a.\n",
    "        other_s_next = [s_next_calc(s, i)\n",
    "                    for i in range(len(A)) if i is not a]\n",
    "        if s_next in other_s_next:\n",
    "            return alpha/4\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVgNRD13EUaa"
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "In policy iteration, we define three functions:\n",
    "\n",
    "* policy_evaluation\n",
    "* policy_improvement\n",
    "* policy_iteration\n",
    "\n",
    "Refer to the lecture slides on policy iteration and value iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "1-Kv6DAFEUab",
    "outputId": "83097735-f74e-46c6-fdd8-01d747a6a349",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, S, Pr, alpha, discount, theta, ctr):\n",
    "    \n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    " \n",
    "    for _ in range(ctr):\n",
    "        # chose an initial delta value for the convergence test\n",
    "        delta = '???'\n",
    "        V_prev = V.copy()\n",
    "        for s, _ in np.ndenumerate(S):\n",
    "            \n",
    "            # action by the policy\n",
    "            a = '???'\n",
    "            # update value function for the state s  \n",
    "            V[s] = '???'\n",
    "            # convergece test  \n",
    "            delta = max(delta, abs(V[s] - V_prev[s]))\n",
    "                            \n",
    "        if delta < theta: break\n",
    "    \n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCJav5gtEUab"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(V, S, A, Pr, alpha, discount):\n",
    "    \n",
    "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
    "    policy_stable = True\n",
    "        \n",
    "    for s, _ in np.ndenumerate(S):\n",
    "            \n",
    "        old_action = policy[s]\n",
    "        Q = np.zeros(len(A))\n",
    "        for a in range(len(A)):\n",
    "            # update Q function at state, s, and action, a\n",
    "            Q[a] = '???'\n",
    "            # update policy at state s\n",
    "            policy[s] = '???'\n",
    "\n",
    "        if old_action != policy[s]: policy_stable = False\n",
    "    \n",
    "    return policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v87MzWcIEUab"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(S, A, Pr, alpha, discount, theta, n_eval, plot_enable, plot=None):\n",
    "    \"\"\"\n",
    "    :param list S: set of states\n",
    "    :param list A: set of actions\n",
    "    :param function Pr: transition function\n",
    "    :param float alpha: noise \n",
    "    :param float discount: discount factor\n",
    "    :param float theta: tolerance, which determines when to end iterations\n",
    "    :param int n_eval: number of evaluations\n",
    "    :param plot: list of iteration numbers to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 0\n",
    "    \n",
    "    # For Task 4 ######\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    time = 0\n",
    "    total_time = 0\n",
    "    ###################\n",
    "    \n",
    "    plt.ion()\n",
    "    policy = np.random.randint(0, len(A), (GRID_SIZE, GRID_SIZE))\n",
    "    count=0\n",
    "    \n",
    "    while True:\n",
    "                \n",
    "        start_time = datetime.datetime.now() # For Task 4\n",
    "        \n",
    "        V, delta = policy_evaluation(policy, S, Pr, alpha, discount, theta, n_eval)\n",
    "        policy , policy_stable = policy_improvement(V, S, A, Pr, alpha, discount)\n",
    "\n",
    "        \n",
    "        # For Task 4\n",
    "        end_time = datetime.datetime.now()\n",
    "        time = (end_time - start_time).total_seconds() # calculate time taken [seconds] for one iteration\n",
    "        total_time += time\n",
    "      \n",
    "\n",
    "        # plot intermediate result\n",
    "        if (plot and count+1 in plot) and plot_enable:\n",
    "            plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "        \n",
    "        if delta == 0:\n",
    "            if plot_enable :\n",
    "                plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            break\n",
    "                \n",
    "        if (delta > 0 and delta<=epsilon) or count==max_it or policy_stable:    \n",
    "            if(count == max_it):\n",
    "                print('Policy iteration failed to converge for alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            if(plot_enable):\n",
    "                 plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            break\n",
    "        count=count+1\n",
    "\n",
    "    return V, policy, count, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_feJtvmWEUab"
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "We use the following function for value iteration. See slides starting from 61."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C74aSbLaEUab"
   },
   "outputs": [],
   "source": [
    "def value_iteration(S, A, Pr, alpha, discount, plot_enable, plot=None):\n",
    "    \"\"\"\n",
    "    :param list S: set of states\n",
    "    :param list A: set of actions\n",
    "    :param function Pr: transition function\n",
    "    :param float alpha: noise\n",
    "    :param float discount: discount factor\n",
    "    \"\"\"\n",
    "    # For Task 4 ######\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    time = 0\n",
    "    total_time = 0\n",
    "    ###################\n",
    "    \n",
    "    plt.ion()\n",
    "\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    optimal_policy = np.random.randint(0, len(A), (GRID_SIZE, GRID_SIZE), dtype=int)\n",
    "\n",
    "    count=0\n",
    "    while True:\n",
    "        \n",
    "        start_time = datetime.datetime.now() # For Task 4\n",
    "    \n",
    "        delta = 0 \n",
    "        V_prev = V.copy()\n",
    "        \n",
    "        for s,_ in np.ndenumerate(S):\n",
    "                    \n",
    "            Q = np.zeros(len(A))\n",
    "            for a in range(len(A)):\n",
    "                Q[a] = '???'  # expression for the Q function at state, s, and action, a\n",
    "            V[s] = '???'\n",
    "            delta = max(delta, '???') # set the validation condition for the convergence\n",
    "            #print(delta)\n",
    "            \n",
    "            optimal_policy[s] = np.argmax(Q)\n",
    "        \n",
    "        # For Task 4\n",
    "        end_time = datetime.datetime.now()\n",
    "        time = (end_time - start_time).total_seconds() # calculate time taken [seconds] for one iteration\n",
    "        total_time += time\n",
    "        \n",
    "\n",
    "        # plot current value and optimal policy\n",
    "        if (plot and count+1 in plot) and plot_enable:\n",
    "            plot_value_grid(V, optimal_policy, msg='Value iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            plt.pause(0.1)\n",
    "        if delta <= 0 or count==max_it: \n",
    "            if count == max_it:\n",
    "                print('Value iteration failed to converge for alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            if(plot_enable):\n",
    "                plot_value_grid(V, optimal_policy, msg='Value iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "                plt.pause(0.1)\n",
    "            break\n",
    "        count=count+1\n",
    "    return V, optimal_policy, count, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyld48sUEUab"
   },
   "source": [
    "#### We will use the following utility function to plot the grid with values from V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vx-euSGWEUab"
   },
   "outputs": [],
   "source": [
    "def plot_value_grid(V, policy, msg=\"\"):\n",
    "    fig = plt.figure(figsize=(5, 5), tight_layout=True)\n",
    "    plt.title(msg)\n",
    "    plt.imshow(V)\n",
    "   \n",
    "    quiver_action_dict = [\n",
    "            [1, 0],\n",
    "            [-1, 0],\n",
    "            [0, -1],\n",
    "            [0, 1],\n",
    "            [0, 0]\n",
    "    ]\n",
    "    for k, a in np.ndenumerate(policy):\n",
    "        if(S[k] == 0): # do not print the arrows on walls, to increase readibility\n",
    "            q = plt.quiver(k[1], k[0], quiver_action_dict[a][1], quiver_action_dict[a][0])\n",
    "    #fig.colorbar(q)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX17TgExEUab"
   },
   "source": [
    "## Tasks\n",
    "### A. Find the optimal solution by two methods for  $\\alpha$ = 0 (no noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUGanpXzEUab"
   },
   "source": [
    "### I. Policy Iteration \n",
    "\n",
    "We are using iterative policy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZL7XN7iEUac"
   },
   "outputs": [],
   "source": [
    "n_pol_eval = 100 #number of policy evalutions\n",
    "val2, pol2, pol_max_iter, time_taken = policy_iteration(S, A, Pr, alpha=0, discount=1, theta = 1e-6, n_eval=n_pol_eval, plot_enable = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBac_v5gEUac"
   },
   "source": [
    "### II. Value Iteration\n",
    "\n",
    "Run for 100 iterations. \n",
    "Plot the value function and the optimal policy, at iterations 1, 25, 50, 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-diPfkcFEUac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "val1, pol1, val_max_iter, time_taken = value_iteration(S, A, Pr, alpha=0, discount=1, plot_enable=True, plot=[1, 25, 50, 100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55aV-hYiEUac"
   },
   "source": [
    "Let's visualize these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usH6kFwPEUac"
   },
   "outputs": [],
   "source": [
    "# Plot for policy iteration alpha = 0, k_max = 100, run for 100 iterations\n",
    "plot_value_grid(val2, pol2, msg='Policy Iteration, {} iterations, {} evaluations'.format(pol_max_iter, n_pol_eval))\n",
    "# Plot for value iteration alpha = 0, 100 iterations\n",
    "plot_value_grid(val1, pol1, msg='Value Iteration, {} iterations'.format(val_max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlBjcTl5maO9"
   },
   "source": [
    "### III. Run Policy Iteration and Value Iteration for  five different discounted factors $\\gamma \\in \\{0, 0.1, 0.4, 0.9, 1\\}$ and perform the following tasks\n",
    "\n",
    "1. Explain the change in utilities for different $\\gamma$\n",
    "2. Explain the change in optimal policies for different $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration for ð›¾âˆˆ{0,0.1,0.4,0.9,1} :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_pol_eval = 100 #number of policy evalutions\n",
    "pol_time_list = []\n",
    "for gamma in [0,0.1,0.4,0.9,1] :  \n",
    "    val2, pol2, pol_max_iter, comp_time = policy_iteration(S, A, Pr, alpha=0, discount=gamma, theta=1e-6, n_eval=n_pol_eval, plot_enable = True)\n",
    "    pol_time_list.append(comp_time)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration for ð›¾âˆˆ{0,0.1,0.4,0.9,1} :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_pol_eval = 100 #number of policy evalutions\n",
    "val_time_list = []\n",
    "for gamma in [0,0.1,0.4,0.9,1] :\n",
    "    val1, pol1, val_max_iter, comp_time = value_iteration(S, A, Pr, alpha=0, discount=gamma, plot_enable=True, plot=None)\n",
    "    val_time_list.append(comp_time)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your explanations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0vOpHommhYZ"
   },
   "source": [
    "### IV. Plot $\\gamma$ VS computational time for the given $\\gamma$ 's in Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ð›¾ VS computational time for Policy iteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Computational time [sec]')\n",
    "plt.plot([0,0.1,0.4,0.9,1], pol_time_list)\n",
    "plt.title('Discount VS Computational time for Policy iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ð›¾ VS computational time for Value iteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Computational time [sec]')\n",
    "plt.plot([0,0.1,0.4,0.9,1], val_time_list)\n",
    "plt.title('Discount VS Computational time for Value iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo7GEfU8EUac"
   },
   "source": [
    "### B. Repeat Value and Policy iteration for 10, 20 and 100 iterations with $\\alpha \\in \\{0.2, 0.8\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGYS8OnhEUac"
   },
   "source": [
    "#### I. Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3NvJ-GhEUac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx1, alpha in enumerate(ALPHA):\n",
    "    val, pol, pol_iter, time_taken = policy_iteration(S, A, Pr, alpha=alpha, discount = 1, theta=1e-6, n_eval=100, plot_enable = True, plot=[10, 20, 100])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_XEL-9LEUac"
   },
   "source": [
    "#### II. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6r-yO1YBEUac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx1, alpha in enumerate(ALPHA):\n",
    "    val, pol, val_iter, comp_time = value_iteration(S, A, Pr, alpha = alpha, discount = 1, plot_enable=True, plot=[10, 20, 100])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RiVQFGDEUac"
   },
   "source": [
    "**Summarize insights and your observations in Question B-I and B-II** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2sLSoI6A8EAH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60btItBW7lAV"
   },
   "source": [
    "### C. Explain (up to 5 sentences) the differences between the approaches in HW1 (search, A*) and the approaches in the current assignment (MDP/Value/Policy) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><center> Part-2 </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h1><center> Utility functions to be implemented / PSEUDOCODE </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space: Same as Part-1.\n",
    "<br>\n",
    "State space: Use the same maze as Part-1. You can use the get_maze() function from Part-1.\n",
    "<br><br>\n",
    "dS - size of the maze with dimensions: dS x dS \n",
    "<br>\n",
    "dA - number of actions\n",
    "<br>\n",
    "Goal - goal state.\n",
    "<br><br>\n",
    "\n",
    "PSEUDOCODE: function validState(s) =  returns true if (state, s, is within the maze boundaries) AND (s is NOT in the obstacles)\n",
    "\n",
    "PSEUDOCODE: function BuildMaze(dS, dA, Goal):\n",
    "<br>\n",
    "\n",
    "\t# dynamics tensor with dimensions: |dS| x |dS| x |dA| x |dS| x |dS| x 1, where the \n",
    "\t# dimensions are Sâ‚, Sâ‚‚, A, Sâ‚â€², Sâ‚‚â€². e.g., Sâ‚‚ is the second coordinate of the current state, and Sâ‚â€² is the first coordinate of the state at the next time step. \n",
    "\tPsâ€²_sa = zeros(dS, dS, dA, dS, dS, 1)\n",
    "\n",
    "\t# the reward tensor with the same dimension as the dynamics\n",
    "\t# reward is 0 at the Goal state, -10000 if agent hits the wall, and -1 elsewhere.\n",
    "\tRsâ€²sa  = -ones(dS, dS, dA, dS, dS, 1)\n",
    "\n",
    "\t# iterate over the valid states\n",
    "\tfor s in filter(validState, (x->x.I).(CartesianIndices((dS, dS))))\n",
    "\t\tif s âˆˆ Goal\n",
    "\t\t\tPsâ€²_sa[s..., :, s...] .= 1.0 # all the actions get prob 1 at the goal \n",
    "\t\t\tRsâ€²sa[s..., :, s...]  .= 0.0 # all the actions get reward 0\n",
    "\t\t\tcontinue\n",
    "\t\tend\n",
    "\n",
    "\t\tfor a in Actions # the same action set at each state \n",
    "\t\t\t# if \"next state is valid\" move to it, otherwise stay at place \n",
    "\t\t\tsâ€² = validState(s .+ a[2]) ? s .+ a[2] : s \n",
    "\t\t\tPsâ€²_sa[s..., a[1], sâ€²...] = 1.0\n",
    "\t\tend \n",
    "\tend\n",
    "\t\"sanity test:\" forall a, s : sum_sâ€² Psâ€²_sa = 1 \n",
    "\treturn Psâ€²_sa, Rsâ€²sa\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h1><center> TASKS </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for the definition of global variables, functions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 ###\n",
    "Build your maze and visualize the maze layout on 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 ###\n",
    "\n",
    "Implement the Policy Evaluation (PE) algorithm for a deterministic policy, Ï€. \n",
    "\n",
    "Evaluate a random deterministic policy, Ï€. Plot Value of a random policy on 2D plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 ###\n",
    "Repeat Task 2 with manually setting the optimal actions in the radius of 2 states from the goal state.\n",
    "Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 ###\n",
    "Implement the Policy Iteration (PI) Algorithm, and find the optimal policy $Ï€^*$.\n",
    "Visualize the optimal value function, V_i, on a 2D plot at 3 different iterations, i, of PI.\n",
    "Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Question (10 points to HW2, if done correclty)\n",
    "Train the agent to find the goal in your maze from samples, using the Q-Learing Algorithm. The agent neither knows $P(s'\\mid s, a)$ nor $R(s', s, a)$, but rather collects samples by running in the maze and learn ${Q}(s, a)$ from these samples. \n",
    "### Tasks\n",
    "\n",
    "a) Derive optimal Q-function, ${Q}^*(s, a)$, using the Q-learning algorithm. \n",
    "\n",
    "a) Run your agent with the optimal policy, $\\pi^*(s)$, starting from three random states and visialize three corresponding paths in your maze.\n",
    "\n",
    "b) Visualize optimal value function, ${V}^*(s)$ in your two dimentional maze. Compare it with value function in Task 4. Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
